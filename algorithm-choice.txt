I chose the algorithm: UCB1 with reward normalization.
The rewards violate UCB1's [0,1] requirement, with maximum observed rewards near 5.0. 
However, as discussed in the course materials, UCB1 can be applied by normalizing rewards (dividing by a constant c when rewards are in [0,c]). 
Since 99.9% of rewards fall below 4.0, capping and normalizing by 4.0 preserves nearly all reward information while enabling UCB1's proven regret bounds.
UCB1's upper confidence bound mechanism will efficiently identify Arm 2 as optimal (mean 0.93 vs 0.82 and 0.57 for other arms) while maintaining sufficient exploration as demonstrated in course examples.